{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Echo: Ambient Clinical Intelligence","text":""},{"location":"#core-problem","title":"Core Problem","text":"<p>Healthcare systems are burdened by documentation overload. We are losing physicians to administrative burnout: doctors spend on average two hours managing Electronic Medical Records (EMR) for every one hour spent listening to patients. </p> <p>For patients, the current system is equally broken. Medical instructions are hard to follow, and the formal clinical language generated by classic hospital systems creates a wall of confusion and anxiety.</p> <p>While AI \"scribes\" exist, they lack the one feature absolutely required in medicine: Trust. Standard wrappers over legacy language models hallucinate facts. In healthcare, hallucinating a diagnosis or a drug dosage is dangerous. Furthermore, transcribing and streaming raw Protected Health Information (PHI/PII) to unverified LLM endpoints violates strict privacy frameworks.</p>"},{"location":"#echo-solution","title":"Echo Solution","text":"<p>Echo - Medical AI Companion is a deterministic, compliance-first ambient engine. It is designed to completely eliminate documentation labor while returning agency and understanding to the patient.</p> <p>Echo listens passively to the doctor-patient dialogue and automatically produces twin documents:</p> <ol> <li>Structured EMR Report: A strictly typed JSON representation, output to formal PDF format, ready to be ingested by the hospital database.</li> <li>Simplified Patient App: A translated, layman-friendly summary sent instantly to the patient's companion application.</li> </ol>"},{"location":"#technical-foundation","title":"Technical Foundation","text":"<p>Echo is built upon two distinct engineering pillars:</p>"},{"location":"#1-zero-hallucination-pipeline","title":"1. Zero-Hallucination Pipeline","text":"<p>Echo text extraction utilizes the native OpenAI Structured Outputs API to enforce strict Pydantic parsing natively within the LLM inference step. We do not prompt the model to generate free-text prose. Instead, we force the AI to produce isolated clinical facts. Every single extracted fact must mathematically cite a perfect string match (an \"exact quote\") from the raw audio transcript. The Python backend then runs a deterministic substring check against the transcript. If the quote doesn't exist, the extraction is rejected. This guarantees total auditability.</p>"},{"location":"#2-opaque-pointer-architecture","title":"2. Opaque Pointer Architecture","text":"<p>Patient data privacy is guaranteed. General healthcare records from the national EESZT database are never fully loaded into our backend. Instead, we utilize Opaque Pointers\u2014generic document identifiers\u2014that Echo identifies during speech. These IDs are rendered on the patient's device as secure portal hyperlinks, ensuring the AI itself never ingests historical personal records.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>To run the Echo platform locally:</p> <pre><code>git clone https://github.com/GregoJaca/GDE-MIT-mesh.git\ncd GDE-MIT-mesh\n./start.sh\n</code></pre> <p>Navigate to <code>http://localhost:5173</code>.</p>"},{"location":"architecture/","title":"Architecture: Zero-Hallucination Engine","text":"<p>Echo's paramount engineering directive is absolute factual accuracy. We transform noisy human dialogue into deterministic, database-safe records. This is achieved via our Schema-Enforced Extraction pipeline, powered natively by Azure OpenAI Structured Outputs.</p>"},{"location":"architecture/#pipeline-flow","title":"Pipeline Flow","text":"<pre><code>graph TD\n    classDef default fill:#111,stroke:#333,stroke-width:1px,color:#eee;\n\n    A[Doctor Browser: MediaRecorder / Audio Blob] --&gt;|POST /api/v1/generate-draft| B(Backend Ingestion)\n    B --&gt; C{Speech-to-Text Transcription}\n    C --&gt;|Raw Transcript| D[PII Scrubbing Layer]\n    D --&gt; E[Azure OpenAI API]\n\n    subgraph Zero-Hallucination Enforcement\n        E --&gt;|Native Structured Output| F(Pydantic Schema Parser)\n        F --&gt; G[Python Deterministic Guardrail]\n        G -- Substring Missing --&gt; H[Strip Hallucination]\n        G -- Substring Match --&gt; I[Strict Clinical JSON]\n    end\n\n    I --&gt; J[Frontend: Doctor Review &amp; Audit]\n    J --&gt;|POST /api/v1/finalize-report| K(Finalizing Engine)\n\n    K --&gt; L[Hospital EMR Representation]\n    K --&gt; M[Patient App Summary Payload]</code></pre>"},{"location":"architecture/#implementing-zero-hallucination","title":"Implementing Zero-Hallucination","text":"<p>Traditional LLMs generate tokens probabilistically. We bypass this by forcing the model to reply exclusively via OpenAI's <code>beta.chat.completions.parse</code> method, which guarantees the structure physically matches our <code>pydantic</code> schemas.</p> <pre><code># snippet from backend/app/models/llm_schemas.py\nclass ClinicalDraftJson(BaseModel):\n    chief_complaints: List[ClinicalFinding] = Field(\n        ..., description=\"Patient's primary reasons for the visit.\"\n    )\n    assessments: List[ClinicalFinding] = Field(...)\n    actionables: List[ActionableItem] = Field(...)\n</code></pre> <p>The core of the architecture lies inside the sub-schemas. Every generated clinical claim requires a verified proof of origin:</p> <pre><code>class ClinicalFinding(BaseModel):\n    finding: str = Field(..., description=\"The clinical claim.\")\n    exact_quote: str = Field(\n        ..., \n        description=\"CRITICAL: The literal, perfect 1-to-5 word substring from the transcript.\"\n    )\n</code></pre> <p>Once the LLM returns the parsed Pydantic object, our backend executes a Deterministic Guardrail (inside <code>ZeroHallucinationPipeline.validate_quotes</code>). This is a zero-tolerance Python validation step:</p> <pre><code># The Deterministic Guardrail Check\nexists = scrubbed_transcript.find(quote) != -1\nif not exists:\n    logger.warning(f\"Guardrail Trip: Hallucinated quote stripped: '{quote}'\")\n</code></pre> <p>If the model hallucinates a fact\u2014meaning the specific word was never physically spoken\u2014the substring check fails (<code>-1</code>), and the entire extraction node is dropped from the report. Because this validation uses native <code>string.find()</code>, it is computationally trivial, requires zero additional LLM latency, and is mathematically infallible. This surfaces an unbroken, transparent audit trail directly to the physician in the frontend UI.</p>"},{"location":"backend/","title":"Backend Pipeline","text":"<p>Echo is powered by an edge-optimized FastAPI Python architecture. We use SQLAlchemy for data persistence and Azure OpenAI for high-precision inference, utilizing the <code>beta.chat.completions.parse</code> Structured Outputs API.</p>"},{"location":"backend/#pipeline-execution","title":"Pipeline Execution","text":"<p>The system explicitly decouples AI extraction from database commitment, enforcing our Zero-Hallucination strategy by ensuring a physician reviews all output before persistence.</p>"},{"location":"backend/#1-post-apiv1generate-draft","title":"1. <code>POST /api/v1/generate-draft</code>","text":"<p>This is the core execution endpoint handling the ambient audio. - Takes the raw binary audio blob. - Connects to the transcription engine. - Pulls Opaque Pointers from the database via <code>db_service.py</code>. - Dispatches to Azure OpenAI, strictly enforcing Pydantic models. - Executes the deterministic String-Match Guardrail against the exact quotes directly in Python. - Returns an aggregated dictionary to the client without executing any database mutations.</p>"},{"location":"backend/#2-post-apiv1finalize-report","title":"2. <code>POST /api/v1/finalize-report</code>","text":"<p>This is the commit endpoint. - Accepts the physician-audited JSON structure. - Renders the structured data into a pristine, hospital-grade PDF via heavily optimized HTML-to-PDF pipelining (<code>wkhtmltopdf</code>). - Formalizes the database relationships, saving the PDF trajectory and Markdown blobs to the patient history.</p>"},{"location":"backend/#intelligence-orchestration","title":"Intelligence Orchestration","text":"<p>Inside <code>app/services/orchestrator.py</code>, the core orchestration flows sequentially, invoking the <code>ZeroHallucinationPipeline</code>.</p> <pre><code># The core zero-hallucination executor\nscrubbed_transcript, token_map = scrubber.scrub(raw_transcript)\n\n# 1. Structure the extraction directly into Pydantic via Azure OpenAI\nthought_process = pipeline.generate_clinical_report(scrubbed_transcript, system_context)\n\n# 2. Run deterministic Python string check against exact_quote\nvalidated_clinical_dict = pipeline.validate_quotes(thought_process, scrubbed_transcript)\n\n# 3. Request multi-lingual patient summary\npatient_summary_dict = pipeline.generate_patient_summary(...)\n\n# 4. Re-hydrate PII\nhydrated_clinical = scrubber.hydrate_dict(validated_clinical_dict, token_map)\nreturn hydrated_clinical, ...\n</code></pre> <p>The strict segregation of raw inference parsing from the deterministic Python validation completely eliminates the reliance on the LLM to govern its own factual accuracy. The API response boundary is always verified, identical, and safe across every consultation.</p>"},{"location":"frontend/","title":"Frontend Ecosystem","text":"<p>Echo\u2019s user interfaces are built with React, TypeScript, Vite, and Tailwind CSS.</p> <p>We target two distinct environments: - Doctor Dashboard: The ambient, browser-based scribe companion. - Patient Dashboard: A mobile-optimized app simulating the patient portal.</p>"},{"location":"frontend/#design-philosophy-zen-ux","title":"Design Philosophy: Zen UX","text":"<p>Medical UI is notoriously heavy, cluttered with dense tables and intrusive borders. Echo actively rejects this paradigm in favor of a strict Zen UX: - Monochrome Integrity: We operate exclusively on a crisp black, white, and zinc palette.  - Borderless Topography: We use ample whitespace to separate content logic. Formal lines define actionable elements, contrasting starkly with typography-focused data displays. - Progressive Disclosure: Deep, complex data (such as the interactive context modals) is hidden by default and summoned only via immediate user click action.</p>"},{"location":"frontend/#audio-engine","title":"Audio Engine","text":"<p>The heart of the Doctor dashboard is powered by the <code>MediaRecorder</code> API.  When the consultation begins, the browser actively buffers ambient dialogue into memory. Because we optimize chunking with backend <code>ffmpeg</code> parsers, the audio stream scales seamlessly without client-side memory exhaustion.</p>"},{"location":"frontend/#state-architecture","title":"State Architecture","text":"<p>The consultation lifecycle is managed explicitly via a rigidly typed state machine:</p> <pre><code>type FlowState = 'idle' | 'drafting' | 'reviewing' | 'finalizing' | 'done';\n</code></pre> <ul> <li><code>idle</code>: Awaiting recording initialization.</li> <li><code>drafting</code>: The recording has ceased. Echo presents a minimal spinner while asynchronous requests transit to the transcription and backend extraction infrastructure.</li> <li><code>reviewing</code>: The core \"Human-in-the-Loop\" interactive phase. The structured draft is displayed as an editable form. The physician audits facts via the <code>exact_quote</code> tooltips.</li> <li><code>finalizing</code>: The physician commits the edited JSON model upstream.</li> <li><code>done</code>: The hospital-grade PDF is finalized and rendered natively via <code>iframe</code>.</li> </ul>"},{"location":"integration/","title":"Security: EESZT Integration","text":"<p>Standard AI companions demand massive context windows to understand a patient's history. Passing entire national logs (like the Hungarian EESZT) into a third-party LLM introduces enormous compliance and data leakage risks.</p> <p>Echo solves this entirely via the Opaque Pointer Pattern.</p>"},{"location":"integration/#opaque-pointers","title":"Opaque Pointers","text":"<p>Instead of ingesting raw medical histories, Echo operates exclusively on metadata.</p> <ol> <li>Lightweight Seeding: The backend database stores only high-level anonymized metadata and generic <code>system_reference_id</code> strings (e.g., <code>DOC-RAD-202</code> for an active radiology exam).</li> <li>Contextual Injection: When a doctor opens a patient file, the LLM system prompt is injected with a map of these pointers. For example: <code>[DOC-RAD-202: 2024-05-12 X-Ray]</code>.</li> <li>Semantic Correlation: During the clinical conversation, if the doctor references \"your recent X-ray,\" the model automatically recognizes the correlation and embeds the pointer into the generated action plan.</li> </ol>"},{"location":"integration/#patient-experience","title":"Patient Experience","text":"<p>The backend returns the patient's summary formatted in Markdown, preserving these pointers. </p> <p>The React frontend utilizes a custom UI parser (<code>&lt;EesztMarkdown /&gt;</code>) that transforms any bracketed EESZT ID into a sealed, secure portal hyperlink.</p> <pre><code>const renderLink = (id: string, text: string) =&gt; (\n    &lt;a href={`https://eeszt.gov.hu/link/${id}`} target=\"_blank\"&gt;\n      &lt;LinkIcon className=\"w-3 h-3\" /&gt;\n      {text || id}\n    &lt;/a&gt;\n);\n</code></pre> <p>When the patient reads their Echo summary on their device, they see clear, styled buttons mapping directly to their national health records. </p> <p>When clicked, the user leaves the Echo application and handles authentication (\u00dcgyf\u00e9lkapu) securely on the government server. Protected data remains entirely unread by our internal systems.</p>"}]}