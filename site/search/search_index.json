{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Mesh: Ambient Clinical Intelligence","text":""},{"location":"#the-core-problem-why-mesh","title":"The Core Problem: Why Mesh?","text":"<p>Healthcare relies on documentation, but the burden has become unsustainable. On average, physicians spend up to two hours managing Electronic Medical Records (EMR) for every one hour of direct patient care. This administrative overload leads to widespread physician burnout.</p> <p>Simultaneously, patients suffer from communication breakdowns. Studies show that between 40% and 80% of medical information provided by healthcare practitioners is forgotten immediately by patients. Furthermore, attempting to parse an actual EMR note designed for a hospital system leaves navigating patients anxious or confused.</p> <p>While AI \"scribes\" exist (often as wrappers over large language models), they suffer from two fatal flaws that prevent real clinical adoption:</p> <ol> <li>Hallucination: Generative models make things up. In medicine, a hallucinated allergy or medication dosage is catastrophic.</li> <li>Data Privacy: Sending raw Protected Health Information (PHI/PII) to cloud-based LLMs violates strict data governance laws in the EU (GDPR) and US (HIPAA).</li> </ol>"},{"location":"#the-solution-ambient-clinical-intelligence-zero-hallucination","title":"The Solution: Ambient Clinical Intelligence + Zero-Hallucination","text":"<p>Mesh is a production-grade, compliance-first Ambient Clinical Engine designed to eliminate physician documentation burden and empower the patient simultaneously.</p> <p>Our system passively listens to the doctor-patient conversation via a minimalist browser dashboard, and deterministically outputs two perfectly aligned documents:</p> <ol> <li>The EMR Report: A highly structured, strictly-typed JSON block rendered into a formal PDF for the hospital system.</li> <li>The Patient Summary: A layman's translation served directly to the patient's companion portal.</li> </ol>"},{"location":"#technical-wedges","title":"Technical Wedges","text":"<p>By restructuring how we ask the LLM for data, we created two major technical breakthroughs:</p>"},{"location":"#1-zero-hallucination-architecture","title":"1. Zero-Hallucination Architecture","text":"<p>We do not use loose LLM prose generation for clinical facts. Instead, we use a strict Pydantic schema-enforced extraction pipeline. If the AI extracts a finding (e.g., <code>\"Diagnosis: Hypertension\"</code>), it is mathematically forced to cite the <code>exact_quote</code> (the 1-to-4 word substring) directly from the raw audio transcript. The frontend surfaces these quotes via tooltips, allowing doctors to hover over any extracted fact and instantly audit the source truth.</p>"},{"location":"#2-the-opaque-pointer-pattern","title":"2. The \"Opaque Pointer\" Pattern","text":"<p>We solve the data-privacy nightmare. Our backend never downloads actual medical histories or highly sensitive historical files. Instead, it ingests generic EESZT Document IDs (e.g., <code>DOC-RAD-202</code>). </p> <p>When the doctor says, \"I saw your X-Ray from Tuesday\", the AI semantic-matches it to <code>DOC-RAD-202</code> and generates a secure, password-protected hyperlink straight into the Hungarian EESZT portal. The patient clicks the link, authenticates via their national ID (\u00dcgyf\u00e9lkapu), and sees the secure X-Ray. PII never touches our AI backend.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>To run the platform locally, including the FastAPI backend and React frontend:</p> <pre><code># Clone the repository\ngit clone https://github.com/GregoJaca/GDE-MIT-mesh.git\n\n# Boot the entire stack\ncd GDE-MIT-mesh\n./start.sh\n</code></pre> <p>Navigate to <code>http://localhost:5173</code> to access the dashboards.</p>"},{"location":"architecture/","title":"System Architecture: The Zero-Hallucination Engine","text":"<p>Mesh\u2019s primary backend engineering objective is to turn fuzzy, unstructured human dialogue into deterministic, safely typable database records. We achieve this through a process known as Schema-Enforced Extraction, wrapped in an audit layer.</p>"},{"location":"architecture/#the-data-flow","title":"The Data Flow","text":"<pre><code>graph TD\n    A[Doctor records Audio via Browser MediaRecorder] --&gt; B(POST /api/v1/generate-draft)\n    B --&gt; C{OpenAI Whisper}\n    C --&gt;|Transcript String| D[PII Scrubber]\n    D --&gt; E[LangChain / OpenAI gpt-4o-mini]\n    E --&gt;|Structured JSON Extraction| F(Schema Validation via Pydantic)\n    F --&gt;|Validation Passed| G[Draft Response sent to Frontend]\n\n    G --&gt; H((Doctor Edits &amp; Approves via UI))\n\n    H --&gt; I(POST /api/v1/finalize-report)\n    I --&gt; J[Generate Patient Markdown]\n    I --&gt; K[Generate Hospital PDF via wkhtmltopdf]\n    K --&gt; L((Saved to DB))</code></pre>"},{"location":"architecture/#pydantic-core","title":"Pydantic Core","text":"<p>We utilize <code>pydantic</code> to enforce the shape of the LLM output. OpenAI's function-calling mechanics natively support this. By refusing to let the LLM reply with a raw string, we strip away its ability to use \"filler\" vocabulary.</p> <pre><code># From backend/app/models/llm_schemas.py\nclass ClinicalDraftJson(BaseModel):\n    chief_complaints: List[ClinicalFinding] = Field(\n        ..., description=\"List of the patient's primary reasons for the visit.\"\n    )\n    assessments: List[ClinicalFinding] = Field(\n        ..., description=\"The physician's diagnoses or assessments.\"\n    )\n    actionables: List[ActionableItem] = Field(\n        ..., description=\"Specific next steps, prescriptions, or referrals.\"\n    )\n</code></pre>"},{"location":"architecture/#the-audit-trail-exact_quote","title":"The Audit Trail (<code>exact_quote</code>)","text":"<p>The secret to our Zero-Hallucination guarantee lies within the sub-schemas. Every <code>ClinicalFinding</code> must include an <code>exact_quote</code>.</p> <pre><code>class ClinicalFinding(BaseModel):\n    finding: str = Field(..., description=\"The core clinical fact (e.g., 'Hypertension').\")\n    condition_status: str = Field(..., description=\"E.g., newly diagnosed, worsening, stable.\")\n    exact_quote: str = Field(\n        ..., \n        description=\"CRITICAL: The exact, literal 1-to-5 word phrase from the transcript that proves this finding. Must be a perfect substring match.\"\n    )\n</code></pre> <p>The frontend uses this <code>exact_quote</code> to power the Hover UI. Because the LLM understands it will fail the schema validation if it cannot extract a perfect substring match, it drastically reduces the likelihood of it hallucinating facts out of thin air. You cannot provide a substring quote for a word that wasn't spoken.</p>"},{"location":"backend/","title":"The Backend Pipeline: FastAPI &amp; Orchestration","text":"<p>The Mesh backend is a high-performance FastAPI service written in Python 3.10+, utilizing SQLAlchemy for persistence and LangChain for LLM orchestration.</p>"},{"location":"backend/#core-endpoints","title":"Core Endpoints","text":"<p>The system exposes two primary endpoints designed to support the Human-in-the-Loop Zero-Hallucination architecture.</p>"},{"location":"backend/#post-apiv1generate-draft","title":"<code>POST /api/v1/generate-draft</code>","text":"<p>This is the entry point for the ambient audio recording. </p> <ol> <li>Ingestion: Accepts <code>multipart/form-data</code> containing the raw <code>.webm</code> or <code>.wav</code> audio <code>Blob</code>.</li> <li>Transcription: Pushed out asynchronously to either a local Whisper instance or Anyscale/OpenAI Whisper endpoints.</li> <li>Context Hydration: The <code>patientId</code> triggers a database lookup via <code>db_service.py</code> to retrieve EESZT pointers.</li> <li>Draft Generation: Instead of saving directly to the DB, the Orchestrator returns the strictly-typed <code>ClinicalDraftJson</code>, the <code>patient_summary_md</code>, and the <code>actionables</code> array straight to the client.</li> </ol>"},{"location":"backend/#post-apiv1finalize-report","title":"<code>POST /api/v1/finalize-report</code>","text":"<p>This endpoint commits the doctor's reviewed facts to the system of record.</p> <ol> <li>Ingestion: Accepts the JSON payload directly from the Doctor Dashboard review screen.</li> <li>PDF Compilation: Under the hood, we use <code>wkhtmltopdf</code> combined with a beautiful HTML Jinja template (<code>app/templates/report.html</code>) to render the formal hospital EMR document.</li> <li>Storage: Both the Markdown patient summary and the PDF URL are saved to the SQLite database via SQLAlchemy schemas.</li> </ol>"},{"location":"backend/#the-orchestrator-appservicesorchestratorpy","title":"The Orchestrator (<code>app/services/orchestrator.py</code>)","text":"<p>The true intelligence of Mesh lies in the orchestration layer. </p> <pre><code>async def generate_draft(\n    audio: UploadFile,\n    patient_id: str,\n    doctor_id: str,\n    date: str,\n    language: str,\n    fallback_transcript: Optional[str] = None\n) -&gt; DraftResponse:\n    # 1. Transcribe the audio\n    transcript_text = await transcribe_audio(audio)\n\n    # 2. Fetch the \"Opaque Pointers\" from the DB\n    patient_context_str = fetch_patient_context_str(patient_id)\n\n    # 3. Extract the Structured Data (Pydantic Schema Enforced)\n    clinical_dict = generate_extraction_data(transcript_text, patient_context_str)\n\n    # 4. Generate the layman's Patient Summary\n    patient_summary_md = generate_patient_summary(\n        transcript=transcript_text, \n        clinical_dict=clinical_dict, \n        patient_context=patient_context_str,\n        language=language\n    )\n\n    return DraftResponse(...)\n</code></pre> <p>By decoupling extraction from finalization, Mesh guarantees that no AI artifact ever enters the patient's legal medical record without explicit human sign-off.</p>"},{"location":"frontend/","title":"Frontend Ecosystem: React &amp; Zen UX","text":"<p>The Mesh frontend is built on React, TypeScript, Tailwind CSS, and Vite. </p> <p>It serves two distinct distinct environments: - <code>/doctor</code>: The Ambient Scribe dashboard. - <code>/patient</code>: The mobile-optimized patient portal.</p>"},{"location":"frontend/#the-design-philosophy-minimalist-zen","title":"The Design Philosophy: Minimalist Zen","text":"<p>Medical interfaces are notoriously cluttered, contributing directly to cognitive load and screen fatigue. Mesh utilizes a strict Zen UX paradigm: - Monochrome &amp; Muted: Core interactions are black, white, and zinc.  - Borderless: Cards and containers use subtle whitespace and soft background colors (<code>bg-zinc-50</code>) instead of harsh borders. - Progressive Disclosure: Complex data (like the EESZT document viewer modal) only appears when explicitly requested.</p>"},{"location":"frontend/#the-audio-capture-pipeline","title":"The Audio Capture Pipeline","text":"<p>The core mechanism of the Doctor Dashboard is the <code>MediaRecorder</code> hook.</p> <ol> <li>Capturing: When the doctor presses \"Record,\" a native browser webhook requests microphone access. </li> <li>Buffering: Audio chunks are buffered locally in the browser memory. Because our backend uses <code>ffmpeg</code> and OpenAI Whisper <code>chunked</code> processing, we can handle audio files that last up to an hour.</li> <li>Drafting: When the doctor presses \"Stop\", the frontend packages the binary <code>Blob</code> into a <code>FormData</code> object and POSTs it to the <code>/generate-draft</code> endpoint.</li> </ol>"},{"location":"frontend/#state-management-doctordashboardtsx","title":"State Management (<code>DoctorDashboard.tsx</code>)","text":"<p>Managing the transition between raw audio, the LLM draft, and the finalized PDF is handled via a strict State Machine <code>flowState</code>:</p> <pre><code>const [flowState, setFlowState] = useState&lt;'idle' | 'drafting' | 'reviewing' | 'finalizing' | 'done'&gt;('idle');\n</code></pre> <ul> <li><code>idle</code>: The microphone is ready, waiting for the doctor.</li> <li><code>drafting</code>: An overlay spinner appears while the backend processes the audio and LLM extraction.</li> <li><code>reviewing</code>: The UI splits. The doctor is presented with an interactive form pre-populated by the LLM's <code>ClinicalDraftJson</code>. They can type, edit, and hover over findings to see the <code>exact_quote</code> Audit Trail.</li> <li><code>finalizing</code>: The edited JSON is posted back to the server.</li> <li><code>done</code>: The PDF <code>iframe</code> renders inline.</li> </ul>"},{"location":"integration/","title":"EESZT Integration: The \"Opaque Pointer\" Pattern","text":"<p>Handling historical health records presents a severe security and privacy bottleneck. To generate an accurate summary, an AI theoretically needs the complete medical history of the patient. However, extracting entire EESZT (Elektronikus Eg\u00e9szs\u00e9g\u00fcgyi Szolg\u00e1ltat\u00e1si T\u00e9r) logs and piping them into an LLM context window exposes hospitals to massive HIPAA/GDPR liabilities. </p> <p>Mesh circumvents this via the \"Opaque Pointer\" Pattern.</p>"},{"location":"integration/#how-it-works","title":"How It Works","text":"<ol> <li>Database Seed: The Mesh database does not store the full text of a patient's historical medical records. It only stores high-level, anonymized metadata alongside a generic <code>system_reference_id</code> (e.g., <code>DOC-DERM-001</code>, representing a previous Dermatology consult).</li> <li>Context Injection: When the Doctor opens \"Jane Doe's\" profile, the backend fetches Jane's available document pointers and secretly prepends them to the LLM's system prompt.</li> <li>Example Injection: <code>\"Patient has the following historical records available in EESZT: [DOC-DERM-001 - 2024-05-12 - Dermatology Consult]\"</code></li> <li>Semantic Matching: During the live consultation, the doctor says: \"I saw your dermatology consult from May...\".</li> <li>Extraction: The LLM, instructed to look for correlations, realizes the spoken text matches the injected metadata. It outputs an <code>actionable</code> or <code>summary_bullet</code> that embeds the pointer.</li> </ol>"},{"location":"integration/#markdown-rendering-frontend","title":"Markdown Rendering (Frontend)","text":"<p>The true magic happens on the patient's device. </p> <p>The API returns the Patient Summary in Markdown format, peppered with our custom syntax:  <code>Your [DOC-DERM-001] was reviewed today...</code></p> <p>The React Frontend utilizes a custom <code>&lt;EesztMarkdown /&gt;</code> component. This component intercepts any bracketed document ID and transforms it into a functional, styled hyperlink:</p> <pre><code>// frontend/src/components/shared/EesztMarkdown.tsx\nconst renderLink = (id: string, text: string) =&gt; (\n    &lt;a \n      href={`https://eeszt.gov.hu/link/${id}`} \n      target=\"_blank\" \n      rel=\"noopener noreferrer\"\n      className=\"inline-flex items-center gap-1 font-mono text-xs...\"\n    &gt;\n      &lt;Link className=\"w-3 h-3\" /&gt;\n      {text || id}\n    &lt;/a&gt;\n);\n</code></pre>"},{"location":"integration/#the-result","title":"The Result","text":"<p>The patient sees a beautiful, blue button: <code>[View Dermatology Consult in EESZT]</code>.  When they click it, they are routed completely out of the Mesh ecosystem and into the secure Hungarian government portal, where they authenticate securely to view the highly sensitive file.</p> <p>PII is never leaked, never stored, and never digested by the LLM.</p>"}]}